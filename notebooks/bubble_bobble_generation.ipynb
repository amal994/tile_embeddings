{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "quiet-hierarchy",
   "metadata": {},
   "source": [
    "## PCGML Level Generation using Tile Embeddings\n",
    "\n",
    "In this notebook, we will walk you through the step-by-step level generation for the game Bubble Bobble. \n",
    "\n",
    "### BIRD'S EYE VIEW: \n",
    "1. We first discuss the pre-requisties in terms of data generation.\n",
    "2. Data loading and preparation\n",
    "3. Model definition and training\n",
    "4. Level generation and visualisation\n",
    "\n",
    "#### 1. Pre-requisties:\n",
    "1. Convert the dataset levels to embedded level representation. For converting the Bubble Bobble levels, execute the code rep_no_affordances.py in the terminal. This will transform and save the levels in the directory data/unified_rep/Bubble_Bobble as shown in the figure\n",
    "\n",
    "<img src=\"images/level_representation.png\">\n",
    "\n",
    "Let's start by importing a whole bunch of python libraries that we will need for the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "religious-teddy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    Activation,\n",
    "    Dropout,\n",
    "    Input,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    BatchNormalization,\n",
    "    LeakyReLU,\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    Add,\n",
    ")\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing import sequence, image\n",
    "from keras.preprocessing.image import load_img, save_img\n",
    "from keras.preprocessing.image import array_to_img, img_to_array\n",
    "from keras import callbacks\n",
    "from keras import regularizers\n",
    "import glob\n",
    "from collections import Counter\n",
    "from PIL import Image, ImageOps\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-olive",
   "metadata": {},
   "source": [
    "### 1. Load encoder-decoder model and embedded level representations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "excellent-vehicle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Dictionary Loaded\n",
      "The feature dictionary has size 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Features'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['block', 'breakable', 'climbable', 'collectable', 'element',\n",
       "       'empty', 'hazard', 'moving', 'openable', 'passable', 'pipe',\n",
       "       'solid', 'wall'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Entire Autoencoder Model from the Disk\n",
      "Loaded Encoder Model from the Disk\n",
      "Loaded Decoder Model from the Disk\n"
     ]
    }
   ],
   "source": [
    "# load the multilabel binarizer\n",
    "with open(\"../model/model_tokenizer.pickle\", \"rb\") as handle:\n",
    "    mlb = pickle.load(handle)\n",
    "print(\"Feature Dictionary Loaded\")\n",
    "total_features = len(mlb.classes_)\n",
    "print(\"The feature dictionary has size\", total_features)\n",
    "display(\"Features\", mlb.classes_)\n",
    "\n",
    "# load entire autoencoder architecture\n",
    "json_file = open(\"../model/autoencoder_model_test.json\", \"r\")\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "ae_sep_output = model_from_json(loaded_model_json)\n",
    "ae_sep_output.load_weights(\"../model/autoencoder_model_test.h5\")\n",
    "print(\"Loaded Entire Autoencoder Model from the Disk\")\n",
    "\n",
    "# load the encoding architecture and weights\n",
    "json_file = open(\"../model/encoder_model_test.json\", \"r\")\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "encoding_model = model_from_json(loaded_model_json)\n",
    "encoding_model.load_weights(\"../model/encoder_model_test.h5\")\n",
    "print(\"Loaded Encoder Model from the Disk\")\n",
    "\n",
    "# load the decoding architecture and weights\n",
    "json_file = open(\"../model/decoder_model_test.json\", \"r\")\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "decoding_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "decoding_model.load_weights(\"../model/decoder_model_test.h5\")\n",
    "print(\"Loaded Decoder Model from the Disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-packaging",
   "metadata": {},
   "source": [
    "### 2.1 Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aging-quarterly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Number of paths read :  95\n",
      "Printing Sample paths  ['../data/unified_rep/bubble_bobble/round63.pickle', '../data/unified_rep/bubble_bobble/round71.pickle', '../data/unified_rep/bubble_bobble/round12.pickle', '../data/unified_rep/bubble_bobble/round47.pickle', '../data/unified_rep/bubble_bobble/round59.pickle']\n"
     ]
    }
   ],
   "source": [
    "# load the embedded level representations\n",
    "save_dir=\"../data/unified_rep/bubble_bobble/\"\n",
    "embedding_paths=glob.glob(save_dir+\"*.pickle\")\n",
    "print(\"\\nTotal Number of paths read : \",len(embedding_paths))\n",
    "print(\"Printing Sample paths \",embedding_paths[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "exotic-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pickle(pickle_path):\n",
    "    '''\n",
    "    This function loads the pickle file\n",
    "    \n",
    "    Input\n",
    "    pickle_path: path of the pickle file\n",
    "    Output\n",
    "    Loads and returns the stored pickle file\n",
    "    \n",
    "    '''\n",
    "    with open(pickle_path, \"rb\") as handle:\n",
    "        level_pickle = pickle.load(handle)\n",
    "    return level_pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-cycling",
   "metadata": {},
   "source": [
    "### 2.2 Data Preparation\n",
    "\n",
    "In this step we build the training batches for model training. A high level architecture of our LSTM generator can be visualised as:\n",
    "\n",
    "<div>\n",
    "<img src=\"images/lstm_training.png\", width=80%>\n",
    " </div>\n",
    " \n",
    "#### Highlights of the architecture:\n",
    "We adapt the work by [Summerville and Mateas](https://arxiv.org/pdf/1603.00930) for this implementation.\n",
    "1. The input to the LSTM is a (156 * 256) representation of a level. This representation is obtained by transforming (16* 16) pixel tiles of (192 * 208) pixel level image to a (12 * 13 * 256) embedding representation. Further this representation is unrolled column-wise to obtain a final representation of dimension (156 * 256). \n",
    "2. Column Array - For a (156 * 256) embedding representation, we generate a corresponding column weight array of size (156,). The column character is incremented after every three columns.\n",
    "3. EOS and SOS are *start of sequence* and *end of sequence* tokens respectively.\n",
    "4. *N* is number of history tiles considered in the implementation. We consider last 6 i.e. 78 tiles.\n",
    "5. Given the history of last 6 columns, we generate next 6 columns. This is denoted by paramter *next_N* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "indoor-malaysia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the column data is (7410, 78, 256)\n",
      "Shape of the input data is (7410, 78, 256)\n",
      "Shape of the target data prepared is (7410, 78, 256)\n"
     ]
    }
   ],
   "source": [
    "#generate a column array\n",
    "col=0\n",
    "level_col=[]\n",
    "for i in range(13):\n",
    "    if i%3==0:\n",
    "        col+=1\n",
    "    for j in range(12):\n",
    "        level_col.append(col)\n",
    "level_col=np.array(level_col)\n",
    "\n",
    "# define start and end token for the sequence\n",
    "sos_array=[9] * 256\n",
    "sos_array=np.array(sos_array).reshape(1,256)\n",
    "eos_array=[5] * 256\n",
    "eos_array=np.array(eos_array).reshape(1,256)\n",
    "\n",
    "#building training batches\n",
    "N=78\n",
    "next_N=78\n",
    "training_input_batch=[]\n",
    "training_target_batch=[]\n",
    "training_column_batch=[]\n",
    "\n",
    "for embed_path in embedding_paths:\n",
    "    \n",
    "    level_identifier=embed_path.split(\"/\")[-1].split('.')[0]\n",
    "    level_embedding=get_pickle(embed_path)\n",
    "    level_column=level_col\n",
    "\n",
    "    for idx in range(len(level_embedding)-next_N):\n",
    "        if idx>N:\n",
    "            low_bound=idx-N\n",
    "            data_i=level_embedding[low_bound:idx]\n",
    "            data_t=level_embedding[idx:idx+next_N]\n",
    "            data_c=level_column[low_bound:idx]\n",
    "            assert data_i.shape==(N,256)\n",
    "            assert data_t.shape==(next_N,256)\n",
    "        else:\n",
    "            pad_length=N-idx\n",
    "            pad_array=np.zeros((pad_length,256))\n",
    "            data_i=np.concatenate((pad_array,level_embedding[:idx]),axis=0)\n",
    "            data_t=level_embedding[idx:idx+next_N]\n",
    "            \n",
    "            pad_array1=np.zeros((pad_length,))\n",
    "            data_c=np.concatenate((pad_array1,level_column[:idx]),axis=0)\n",
    "            assert data_i.shape==(N,256)\n",
    "            assert data_t.shape==(next_N,256)\n",
    "        training_column_batch.append(data_c)\n",
    "        training_input_batch.append(data_i)\n",
    "        training_target_batch.append(data_t)\n",
    "        \n",
    "training_column_batch=np.array(training_column_batch)        \n",
    "training_input_batch=np.array(training_input_batch)\n",
    "training_target_batch=np.array(training_target_batch)\n",
    "\n",
    "for cp in training_column_batch:\n",
    "    assert cp.shape==(next_N,)\n",
    "\n",
    "training_column_batch=training_column_batch.astype(int)\n",
    "\n",
    "input_one_hot_col=np.zeros((len(training_column_batch),N,256))\n",
    "\n",
    "for i, d in enumerate(training_column_batch):\n",
    "    for t,word in enumerate(d):\n",
    "#         print(\"i\",i,\"t:\",t,\"word:\",word,\"d:\",d)\n",
    "        input_one_hot_col[i,t,word]=1\n",
    "\n",
    "print(\"Shape of the column data is\",input_one_hot_col.shape)\n",
    "print(\"Shape of the input data is\",training_input_batch.shape)\n",
    "print(\"Shape of the target data prepared is\",training_target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-correspondence",
   "metadata": {},
   "source": [
    "### 3. Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "virgin-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_batch(training_input_batch, training_target_batch,input_one_hot_col,chunk_size=25):\n",
    "                       \n",
    "    index_tracker=0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        batch_input=training_input_batch[index_tracker:index_tracker+chunk_size]\n",
    "        batch_targets=training_target_batch[index_tracker:index_tracker+chunk_size]\n",
    "        batch_column_in=input_one_hot_col[index_tracker:index_tracker+chunk_size]\n",
    "        \n",
    "        batch_target_in=[]\n",
    "        batch_target_out=[]\n",
    "        \n",
    "        for target in batch_targets:\n",
    "            target_in=np.concatenate((sos_array,target))\n",
    "            target_out=np.concatenate((target,eos_array))\n",
    "            batch_target_in.append(target_in)\n",
    "            batch_target_out.append(target_out)\n",
    "          \n",
    "        batch_target_in=np.array(batch_target_in)\n",
    "        batch_target_out=np.array(batch_target_out)\n",
    "        batch_column_in =np.array(batch_column_in)   \n",
    "        \n",
    "        index_tracker +=chunk_size\n",
    "        if index_tracker>len(training_input_batch):\n",
    "            import random\n",
    "            index_tracker=0\n",
    "            c = list(zip(training_input_batch, training_target_batch,input_one_hot_col))\n",
    "            random.shuffle(c)\n",
    "            training_input_batch, training_target_batch, input_one_hot_col = zip(*c)\n",
    "\n",
    "        \n",
    "        yield [batch_input,batch_column_in,batch_target_in],batch_target_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alternate-smooth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 78, 256)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 78, 256)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_layer1 (LSTM)              [(None, 78, 128), (N 197120      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_layer2 (LSTM)              [(None, 78, 128), (N 197120      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 79, 256)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 128)          0           lstm_layer1[0][1]                \n",
      "                                                                 lstm_layer2[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 128)          0           lstm_layer1[0][2]                \n",
      "                                                                 lstm_layer2[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_layer3 (LSTM)              [(None, 79, 128), (N 197120      input_3[0][0]                    \n",
      "                                                                 add[0][0]                        \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Dense2 (Dense)                  (None, 79, 256)      33024       lstm_layer3[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 624,384\n",
      "Trainable params: 624,384\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hist_seq=Input(shape=(N,256))\n",
    "lstm_out1,h1,c1=LSTM(units=128,return_state=True, return_sequences=True,name=\"lstm_layer1\")(hist_seq)\n",
    "hidden_states1=[h1,c1]\n",
    "hist_model=Model(inputs=[hist_seq],outputs=[lstm_out1,h1,c1])\n",
    "\n",
    "col_seq=Input(shape=(N,256))\n",
    "lstm_out2,h2,c2=LSTM(units=128,return_state=True, return_sequences=True,name=\"lstm_layer2\")(col_seq)\n",
    "hidden_states2=[h2,c2]\n",
    "col_model=Model(inputs=[col_seq],outputs=[lstm_out2,h2,c2])\n",
    "\n",
    "hidden_h=Add()([h1,h2])\n",
    "hidden_c=Add()([c1,c2])\n",
    "hidden_states=[hidden_h,hidden_c]\n",
    "\n",
    "text_seq=Input(shape=(next_N+1,256))\n",
    "lstm_out3,h3,c3=LSTM(units=128,return_state=True,return_sequences=True,name='lstm_layer3')(text_seq, initial_state=hidden_states)\n",
    "decoder_dense = Dense(256, activation='tanh', name = \"Dense2\")(lstm_out3)\n",
    "\n",
    "model=Model(inputs=[hist_seq,col_seq,text_seq],outputs=[decoder_dense])\n",
    "model.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "portuguese-compromise",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "297/297 [==============================] - 62s 207ms/step - loss: 0.4292\n",
      "Epoch 2/5\n",
      "297/297 [==============================] - 68s 229ms/step - loss: 0.4277\n",
      "Epoch 3/5\n",
      "297/297 [==============================] - 64s 215ms/step - loss: 0.4236\n",
      "Epoch 4/5\n",
      "297/297 [==============================] - 64s 215ms/step - loss: 0.4204\n",
      "Epoch 5/5\n",
      "297/297 [==============================] - 63s 211ms/step - loss: 0.4184\n"
     ]
    }
   ],
   "source": [
    "batch_gen=get_training_batch(training_input_batch, training_target_batch,input_one_hot_col, chunk_size=25)\n",
    "model_hist = model.fit_generator(batch_gen, steps_per_epoch=int(np.ceil(len(training_input_batch) / 25)),epochs=5,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-sheep",
   "metadata": {},
   "source": [
    "### 4.1 Level Generation - Defining the Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "three-techno",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 78, 256)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_layer3 (LSTM)              multiple             197120      input_9[0][0]                    \n",
      "                                                                 input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Dense2 (Dense)                  multiple             33024       lstm_layer3[2][0]                \n",
      "==================================================================================================\n",
      "Total params: 230,144\n",
      "Trainable params: 230,144\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inf_hidden_h=Input(shape=(128,))\n",
    "inf_hidden_c=Input(shape=(128,))\n",
    "\n",
    "decoder_hidden_states=[inf_hidden_h,inf_hidden_c]\n",
    "\n",
    "inf2_input=Input(shape=(next_N,256))\n",
    "inf2_lstm,inf2_h2,inf2_c2=model.get_layer(\"lstm_layer3\")(inf2_input,initial_state=decoder_hidden_states)\n",
    "inf2_dense=model.get_layer(\"Dense2\")(inf2_lstm)\n",
    "\n",
    "inf_model2=Model(inputs=[inf2_input,inf_hidden_h,inf_hidden_c],outputs=[inf2_dense])\n",
    "inf_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "nutritional-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the index to embed and \n",
    "idx2embed_map=get_pickle(save_dir+\"mappings/idx2embed.pickle\")\n",
    "idx2tile_map=get_pickle(save_dir+\"mappings/idx2tile.pickle\")\n",
    "\n",
    "def roll_embedding(level_h,level_w,embedding):\n",
    "    \n",
    "    '''\n",
    "    This function bundles the 2-dimensional string of embedding representations to a 3-dimensional array. \n",
    "    It is necessary to map the generated embedding representation to pixel representation\n",
    "    For instance, a string of (wh*256) can be converted to (h*w*256) where,\n",
    "    h is the height or the number of rows\n",
    "    w is the width or the number of columns \n",
    "    '''\n",
    "    ptr=0\n",
    "    level_embedding=np.zeros((level_h,level_w,256))\n",
    "    for x in range(level_w):\n",
    "        for y in range(level_h):\n",
    "            level_embedding[y][x]=embedding[ptr]\n",
    "            ptr+=1\n",
    "    return level_embedding\n",
    "\n",
    "def visualise_level(test1):\n",
    "    from PIL import Image, ImageOps\n",
    "    image_height,image_width=(test1.shape[0]*16),(test1.shape[1]*16)\n",
    "    image = Image.new(\"RGB\", (image_width,image_height), color=(0, 0,0))\n",
    "    pixels = image.load()\n",
    "\n",
    "    for x in range(test1.shape[1]):\n",
    "        for y in range(test1.shape[0]):\n",
    "\n",
    "            imagetouse = None\n",
    "\n",
    "            embedding=test1[y][x]\n",
    "            v=embedding\n",
    "            list1=t.get_nns_by_vector(v, n, search_k=-1, include_distances=False)\n",
    "            tile_idx=list1[0]\n",
    "            imagetouse=idx2tile_map[tile_idx]\n",
    "            imagetouse=array_to_img(imagetouse)\n",
    "            if not imagetouse==None:\n",
    "                pixelstouse=imagetouse.load()\n",
    "                for x2 in range(0,16):\n",
    "                    for y2 in range(0,16):\n",
    "                        pixels[x*16+x2,y*16+y2]=pixelstouse[x2,y2]                    \n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-montgomery",
   "metadata": {},
   "source": [
    "### 4.2 Level Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "female-tampa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-09 08:04:29.306196: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-09-09 08:04:29.339551: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# number of levels to be generated\n",
    "n_levels=10\n",
    "for i_ptr in range(n_levels):\n",
    "    \n",
    "    N=78\n",
    "    next_N=78\n",
    "\n",
    "    embed_path=embedding_paths[i_ptr]\n",
    "    level_identifier=embed_path.split(\"/\")[-1].split('.')[0]\n",
    "    level_embedding=get_pickle(embed_path)\n",
    "    level_column=level_col\n",
    "\n",
    "    inf_column_batch=[]\n",
    "    inf_input_batch=[]\n",
    "    inf_target_batch=[]\n",
    "\n",
    "    for idx in range(len(level_embedding)-next_N):\n",
    "        if idx>N:\n",
    "            low_bound=idx-N\n",
    "            data_i=level_embedding[low_bound:idx]\n",
    "            data_t=level_embedding[idx:idx+next_N]\n",
    "            data_c=level_column[low_bound:idx]\n",
    "\n",
    "            assert data_i.shape==(N,256)\n",
    "            assert data_t.shape==(next_N,256)\n",
    "        else:\n",
    "            pad_length=N-idx\n",
    "            pad_array=np.zeros((pad_length,256))\n",
    "            data_i=np.concatenate((pad_array,level_embedding[:idx]),axis=0)\n",
    "            data_t=level_embedding[idx:idx+next_N]\n",
    "\n",
    "            pad_array1=np.zeros((pad_length,))\n",
    "            data_c=np.concatenate((pad_array1,level_column[:idx]),axis=0)\n",
    "            assert data_i.shape==(N,256)\n",
    "            assert data_t.shape==(next_N,256)\n",
    "        inf_column_batch.append(data_c)\n",
    "        inf_input_batch.append(data_i)\n",
    "        inf_target_batch.append(data_t)\n",
    "\n",
    "    inf_column_batch=np.array(inf_column_batch)      \n",
    "    inf_column_batch=inf_column_batch.astype(int)\n",
    "    inf_input_batch=np.array(inf_input_batch)\n",
    "    inf_target_batch=np.array(inf_target_batch)\n",
    "\n",
    "    # initialising annoy\n",
    "    vec_list=list(idx2embed_map.values())\n",
    "    len(vec_list)\n",
    "    f = 256\n",
    "    t = AnnoyIndex(f, 'euclidean')\n",
    "    for i in range(len(vec_list)):\n",
    "        t.add_item(i,vec_list[i])\n",
    "    t.build(15)\n",
    "    \n",
    "    generated_level=[]\n",
    "    tile_count=0\n",
    "    col_idx=0\n",
    "    n=2\n",
    "    col_hist=inf_input_batch[0]\n",
    "    col=inf_column_batch[0]\n",
    "    start_input=inf_target_batch[0]\n",
    "    start_input=start_input[:next_N]\n",
    "\n",
    "\n",
    "    column_one_hot=np.zeros((1,N,256))\n",
    "    for i, d in enumerate(col):\n",
    "        column_one_hot[0,i,d]=1\n",
    "    col_hist_in=col_hist.reshape(1,N,256)\n",
    "\n",
    "    start_input_in=start_input.reshape(1,next_N,256)\n",
    "\n",
    "    o1,h1,c1=hist_model.predict(col_hist_in)\n",
    "    o2,h2,c2=col_model.predict(column_one_hot)\n",
    "    oh=Add()([h1,h2])\n",
    "    oc=Add()([c1,c2])\n",
    "    next_col=inf_model2.predict([start_input_in,oh,oc])\n",
    "    \n",
    "    second_half=next_col[0]\n",
    "    \n",
    "    assert second_half.shape==(78,256)\n",
    "    level_embed=roll_embedding(13,6,second_half)\n",
    "    second_v=visualise_level(level_embed)\n",
    "    first_v=ImageOps.mirror(second_v)\n",
    "    \n",
    "    images = [second_v,first_v]\n",
    "    widths, heights = zip(*(i.size for i in images))\n",
    "    total_width = sum(widths)\n",
    "    max_height = max(heights)\n",
    "\n",
    "    new_im = Image.new('RGB', (total_width, max_height))\n",
    "\n",
    "    x_offset = 0\n",
    "    for im in images:\n",
    "        new_im.paste(im, (x_offset,0))\n",
    "        x_offset += im.size[0]\n",
    "    save_img(\"../outputs/Bubble_Bobble/level_\"+str(i_ptr)+\".png\",new_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c5c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
